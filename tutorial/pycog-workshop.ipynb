{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from pycog import tasktools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These RNNs have three layers (one could make more...) of time-varying \"neurons\": input units $\\vec{u}(t)$, recurrent (hidden) units $\\vec{r}(t)$, and output units $\\vec{z}(t)$. The dynamics of the recurrent units are given by:\n",
    "$$ \\tau\\frac{d\\vec{x}}{dt} = -\\vec{x} + W^{rec}\\vec{r} + W^{in}\\vec{u} + \\sqrt{2\\tau\\sigma^{2}_{rec}}\\vec{\\xi} $$\n",
    "$$ \\vec{r} = [\\vec{x}]_{+} $$\n",
    "$$ \\vec{z} = W^{out}\\vec{r} $$\n",
    "where $\\vec{\\xi}$ is a vector of independent gaussian RVs, and $[.]_{+}$ is the rectification function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's decide that task that we want the network to perform. Let's start with something simple. How about a discrimination task? We want the network to signal via one output neuron if the input is less than some threshold $t$ and signal via a different output neuron if the input is greater than that value. To be clear, we have an input $x$ and two outputs $\\vec{y}$, $y_{1}$ and $y_{2}$, we want our network to perform the function $f$, as in $\\vec{y} = f(x)$, with $f$ defined as,\n",
    "\n",
    "$ f(x) = \n",
    "\\begin{cases}\n",
    "[0, r]^{T} & x > t \\\\\n",
    "[r, 0]^{T} & x \\leq t\n",
    "\\end{cases}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide what kind of network we want to use first. Two things are immediately clear: we only need one input and two output neurons. \n",
    "\n",
    "How many neurons do we want for the hidden layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network details\n",
    "nin = 1\n",
    "n = 20\n",
    "nout = 2\n",
    "\n",
    "ei, exc, inh = tasktools.generate_ei(n)\n",
    "ne = len(exc)\n",
    "ni = len(inh)\n",
    "\n",
    "# input\n",
    "visual_input = 0 # label for the input index, we might want to add more later!\n",
    "visual_exc = exc # all excitatory neurons in the hidden layer will receive visual input\n",
    "# but note that the hidden units will not\n",
    "cin = np.zeros((n, nin))\n",
    "cin[visual_exc, visual_input] = 1\n",
    "baseline_input = .2 # noise\n",
    "\n",
    "# output\n",
    "left_resp = 0\n",
    "right_resp = 1\n",
    "cout = np.zeros((nout, n))\n",
    "cout[:, exc] = 1 # output neurons both receive from all exc neurons\n",
    "\n",
    "hi_resp = 1\n",
    "lo_resp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(8)\n",
    "y = x\n",
    "print y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we have a fairly clear trial structure. We want to choose $t$ such that $t$ is in the set of possible $x$ and then train the network to make the correct discrimination by giving it the correct output vector in training. This is done by the \"generate_trial\" function. First, let's consider how many conditions we have:\n",
    "1. $x > t \\rightarrow [0, r]^{T}$\n",
    "2. $x \\leq t \\rightarrow [r, 0]^{T}$\n",
    "\n",
    "Do we want anything else?\n",
    "\n",
    "Something that appears to be useful in training is to assert the stimulus dependence of the network, by inserting trials on which the network is required to not robustly respond, that is, catch trials. \n",
    "\n",
    "The generate_file function has a strict input and output contract, but uses closures to store lots of variables declared globally before it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 20 # ms, we don't need to integrate with too much precision, especially for training -- we can plot with\n",
    "        # higher resolution\n",
    "tau = 100\n",
    "\n",
    "thresh = 10. # where's the discrimination boundary?\n",
    "x_std = 5. # let's give it a somewhat large spread\n",
    "\n",
    "fixation = 500\n",
    "sample = 400\n",
    "decision = 300\n",
    "T = fixation + sample + decision\n",
    "epochs = {'fixation':(0, fixation),\n",
    "          'stimulus':(fixation, fixation + sample),\n",
    "          'decision':(fixation + sample, T),\n",
    "          'T':T}\n",
    "\n",
    "upperbound = thresh*2.\n",
    "scale_x = lambda x: max(min(x, upperbound), 0.)/upperbound\n",
    "# we don't need to declare rng or params here, but we do need to give \"generate_trial\" everything \n",
    "# else it needs to generate a \"trial\" structure in the two conditions we've described\n",
    "def generate_trial(rng, dt, params):\n",
    "    sample_x  = rng.randn()*x_std + thresh\n",
    "    sc_x = scale_x(sample_x)\n",
    "    t, e = tasktools.get_epochs_idx(dt, epochs)\n",
    "    trial = {'t':t, 'epochs':epochs}\n",
    "    \n",
    "    X = np.zeros((len(t), nin))\n",
    "    X[e['stimulus'], :] = sc_x\n",
    "    trial['inputs'] = X\n",
    "    \n",
    "    if sample_x > thresh:\n",
    "        choice = 1\n",
    "    else:\n",
    "        choice = 0\n",
    "    Y = np.zeros((len(t), nout))\n",
    "    M = np.zeros_like(Y)\n",
    "    Y[e['fixation'], :] = lo_resp\n",
    "    Y[e['decision'], choice] = hi_resp\n",
    "    Y[e['decision'], 1-choice] = lo_resp\n",
    "    M[e['fixation']+e['decision'], :] = 1\n",
    "    trial['outputs'] = Y\n",
    "    trial['mask'] = M\n",
    "        \n",
    "    trial['info'] = {'input': sample_x, 'scaled_input':sc_x, 'choice': choice}\n",
    "    return trial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's decide how we'll evaluate the performance of our network and when we'll tell our network to stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there are several ready-made performance metrics in the tasktools file, this is a two\n",
    "# alternative forced choice task, so let's use that one\n",
    "performance = tasktools.performance_2afc\n",
    "\n",
    "# termination criterion\n",
    "target_performance = 90\n",
    "def terminate(performance_history):\n",
    "    return np.mean(performance_history[-5:]) > target_performance\n",
    "\n",
    "nconds = 2\n",
    "n_validation = 100*nconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, now we have all of the pieces that we need. Let's construct and then train the model. So far, we've had to do a lot of the work, now pycog will take over from us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycog import Model\n",
    "\n",
    "model = Model(N=n, Nout=nout, Nin=nin, ei=ei, tau=tau, dt=dt,\n",
    "              generate_trial=generate_trial, n_validation=n_validation, terminate=terminate,\n",
    "              performance=performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = np.random.randint(0, 1000000)\n",
    "model.train('test.pkl', seed=seed, recover=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycog import RNN\n",
    "plot_dt = 1\n",
    "#rnn = RNN('ramping_pathology.pkl', {'dt':plot_dt})\n",
    "rnn = RNN('test.pkl', {'dt':plot_dt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "info = rnn.run(inputs=(generate_trial, {}), seed=np.random.randint(0, 10000))\n",
    "print info\n",
    "f = plt.figure()\n",
    "corr_choice = info['choice']\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "ax.plot(rnn.t, rnn.z[corr_choice], label='target')\n",
    "ax.plot(rnn.t, rnn.z[1 - corr_choice], label='other')\n",
    "ax.legend()\n",
    "ax.set_xlabel('t (ms)')\n",
    "ax.set_ylabel('activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_t = 1000\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "info = rnn.run(T=run_t, seed=np.random.randint(0, 10000))\n",
    "print info\n",
    "ax.plot(rnn.t, rnn.z[0], label='0')\n",
    "ax.plot(rnn.t, rnn.z[1], label='1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, but hmmm, this ramping is a little bit annoying. We should do something more to constrain both output neurons to not respond unless it's the right stimulus. Let's reconsider our trial function. We want to add a third condition, in which the neurons are constrained to fire at as low a rate as possible, and in which there is no stimulus given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catch_prob = 1/3.\n",
    "catch_len = 2000\n",
    "\n",
    "def generate_trial_with_catch(rng, dt, params):\n",
    "    cp = params.get('catch_prob', catch_prob)\n",
    "    sc_x = params.get('samp_x',scale_x(rng.randn()*x_std + thresh))\n",
    "    if rng.rand() < cp:\n",
    "        epochs = {'T':catch_len}\n",
    "        t, e = tasktools.get_epochs_idx(dt, epochs)\n",
    "        trial = {'t':t, 'epochs':epochs}\n",
    "        trial['info'] = {}\n",
    "        trial['inputs'] = np.zeros((len(t), nin))\n",
    "        trial['outputs'] = np.zeros((len(t), nout)) + lo_resp\n",
    "        trial['mask'] = np.ones_like(trial['outputs'])\n",
    "    else:\n",
    "        fixation = 500\n",
    "        sample = 400\n",
    "        decision = 300\n",
    "        T = fixation + sample + decision\n",
    "        epochs = {'fixation':(0, fixation),\n",
    "                  'stimulus':(fixation, fixation + sample),\n",
    "                  'decision':(fixation + sample, T),\n",
    "                  'T':T}\n",
    "        # sc_x = scale_x(sample_x)\n",
    "        t, e = tasktools.get_epochs_idx(dt, epochs)\n",
    "        trial = {'t':t, 'epochs':epochs}\n",
    "    \n",
    "        X = np.zeros((len(t), nin))\n",
    "        X[e['stimulus'], :] = sc_x\n",
    "        trial['inputs'] = X\n",
    "    \n",
    "        if sc_x > (thresh/upperbound):\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = 0\n",
    "        Y = np.zeros((len(t), nout))\n",
    "        M = np.zeros_like(Y)\n",
    "        Y[e['fixation'], :] = lo_resp\n",
    "        Y[e['decision'], choice] = hi_resp\n",
    "        Y[e['decision'], 1-choice] = lo_resp\n",
    "        M[e['fixation']+e['decision'], :] = 1\n",
    "        trial['outputs'] = Y\n",
    "        trial['mask'] = M\n",
    "        \n",
    "        trial['info'] = {'scaled_input':sc_x, 'choice': choice}\n",
    "    return trial\n",
    "\n",
    "model = Model(N=n, Nout=nout, Nin=nin, ei=ei, tau=tau, dt=dt,\n",
    "              generate_trial=generate_trial_with_catch, n_validation=n_validation, \n",
    "              terminate=terminate, performance=performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = np.random.randint(0, 1000000)\n",
    "model.train('test_ps.pkl', seed=seed, recover=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycog import RNN\n",
    "plot_dt = 1\n",
    "#rnn = RNN('ramping_treated_wcheck.pkl', {'dt':plot_dt}, verbose=False)\n",
    "rnn = RNN('test_ps.pkl', {'dt':plot_dt}, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_t = 1000\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "info = rnn.run(T=run_t, seed=np.random.randint(0, 10000))\n",
    "ax.plot(rnn.t, rnn.z[0], label='0')\n",
    "ax.plot(rnn.t, rnn.z[1], label='1')\n",
    "plt.imshow(rnn.Wrec)\n",
    "plt.axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "trial_print = \"\"\"=> settings | dt:        1 ms | threshold: 0.0001\"\"\"\n",
    "def gen_trial(rnn, tfunc, params={}, seed_lim=10000000, suppress_out=True):\n",
    "    info = rnn.run(inputs=(tfunc, params), seed=np.random.randint(0, seed_lim))\n",
    "    if suppress_out:\n",
    "        sys.stdout.write(len(trial_print)*'\\b')\n",
    "        sys.stdout.flush()\n",
    "    return rnn.t, rnn.r, rnn.z, info\n",
    "\n",
    "def single_trial_2afcperf(act, info):\n",
    "    nc = np.argmax(act[:, -1])\n",
    "    corr = nc == info['choice']\n",
    "    return corr\n",
    "\n",
    "def gen_many_trials(rnn, tfunc, n=1, params={}, seed_lim=10000000, perf=None):\n",
    "    if perf is not None:\n",
    "        corr = np.zeros(n)\n",
    "    else:\n",
    "        corr = None\n",
    "    for i in xrange(n):\n",
    "        ts, rs, zs, info = gen_trial(rnn, tfunc, params, seed_lim)\n",
    "        if i == 0:\n",
    "            allrs = np.zeros((n, len(rs), len(ts)))\n",
    "            allzs = np.zeros((n, len(zs), len(ts)))\n",
    "            allinfo = []\n",
    "        allrs[i] = rs\n",
    "        allzs[i] = zs\n",
    "        allinfo.append(info)\n",
    "        if perf is not None:\n",
    "            corr[i] = perf(zs, info)\n",
    "    return ts, allrs, allzs, allinfo, corr\n",
    "    \n",
    "def gen_and_plot_output(rnn, tfunc, params={}, seed_lim=1000000, n=1, perf=None):\n",
    "    ts, rs, zs, info, corr = gen_many_trials(rnn, tfunc, n, params, seed_lim, perf)\n",
    "    f = plt.figure()\n",
    "    corr_choice = info['choice']\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    ax.set_title('input: {}'.format(info['scaled_input']))\n",
    "    ax.plot(ts, zs[corr_choice], label='target')\n",
    "    ax.plot(ts, zs[1 - corr_choice], label='other')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('t (ms)')\n",
    "    ax.set_ylabel('activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'a'*10 \n",
    "sys.stdout.write('\\b'*10 + 'b'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 1, 20)\n",
    "rates = np.zeros_like(xs)\n",
    "tn = 100\n",
    "for i, x in enumerate(xs):\n",
    "    params = {'catch_prob':0, 'samp_x':x}\n",
    "    _, _, _, _, corr = gen_many_trials(rnn, generate_trial_with_catch, n=tn, params=params, \n",
    "                                       perf=single_trial_2afcperf)\n",
    "    rates[i] = np.mean(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "ax.plot(xs, rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the output units is fine, but they don't tell us anything about what the hidden units are doing -- ie, how the task is actually being solved. Let's examine the hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_state = .2\n",
    "right_state = .8\n",
    "n = 100\n",
    "lpar = {'catch_prob':0, 'samp_x':left_state}\n",
    "rpar = {'catch_prob':0, 'samp_x':right_state}\n",
    "\n",
    "ts, lrs, lzs, linfo, _ = gen_many_trials(rnn, generate_trial_with_catch, n=n, params=lpar)\n",
    "ts, rrs, rzs, rinfo, _ = gen_many_trials(rnn, generate_trial_with_catch, n=n, params=rpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lrs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now what we have is a couple big arrays: One when the stimulus is below the threshold (and the network is reliably sensitive to that) and the other when it's above threshold. Let's do the easiest thing first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "ax1 = f.add_subplot(3, 1, 1)\n",
    "_ = ax1.plot(ts, np.mean(lrs, 0)[exc].T)\n",
    "_ = ax1.plot(ts, np.mean(lrs, 0)[inh].T, '--')\n",
    "ax2 = f.add_subplot(3, 1, 2)\n",
    "_ = ax2.plot(ts, np.mean(rrs, 0)[exc].T)\n",
    "_ = ax2.plot(ts, np.mean(rrs, 0)[inh].T, '--')\n",
    "ax3 = f.add_subplot(3, 1, 3)\n",
    "lcorr, lincorr = np.mean(lzs, 0)\n",
    "rincorr, rcorr = np.mean(rzs, 0)\n",
    "_ = ax3.plot(ts, lcorr.T, 'r', ts, lincorr.T, 'b--')\n",
    "_ = ax3.plot(ts, rincorr.T, 'r--', ts, rcorr.T, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this isn't very informative. If we squint, we can generate a strategy that appears to be at work: There appears to be a subpopulation that drives the transient dashed red spike, that is then inhibited while otherwise it is simply allowed to grow (albeit more slowly). When it is the correct answer, it does not reach strength triggering inhibition until later on. So, we have an intuition -- how could we show this more concretely? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(5, 10))\n",
    "ax = f.add_subplot(3, 1, 1)\n",
    "_ = ax.imshow(rnn.Wrec, interpolation='none')\n",
    "ax.set_title('recurrent weights')\n",
    "ax2 = f.add_subplot(3, 1, 2)\n",
    "_ = ax2.imshow(rnn.Wout, interpolation='none')\n",
    "ax3 = f.add_subplot(3, 1, 3)\n",
    "_ = ax3.imshow(rnn.Win.T, interpolation='none')\n",
    "f.tight_layout()\n",
    "print rnn.Wout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "ax1 = f.add_subplot(3, 1, 1)\n",
    "loutmean = np.mean(lrs, 0)[rnn.Wout.sum(0) > .1]\n",
    "routmean = np.mean(rrs, 0)[rnn.Wout.sum(0) > .1]\n",
    "_ = ax1.plot(ts, loutmean.T)\n",
    "ax2 = f.add_subplot(3, 1, 2)\n",
    "_ = ax2.plot(ts, routmean.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we sort by average response to an above threshold stimulus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lrs.shape\n",
    "print epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_slice = np.mean(lrs[:, :, epochs['stimulus'][0]:epochs['decision'][-1]], 0)\n",
    "l_selec = np.mean(l_slice, 1)\n",
    "l_var = np.var(l_slice, 1)\n",
    "\n",
    "r_slice = np.mean(rrs[:, :, epochs['stimulus'][0]:epochs['decision'][-1]], 0)\n",
    "r_selec = np.mean(r_slice, 1)\n",
    "r_var = np.var(r_slice, 1)\n",
    "\n",
    "d_primes = (l_selec - r_selec)/(np.sqrt(r_var + l_var)/2)\n",
    "w_sort = np.argsort(d_primes)\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "print np.nan_to_num(d_primes)\n",
    "ax.hist(np.nan_to_num(d_primes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws = rnn.Wrec\n",
    "sortedws = ws[w_sort, :][:, w_sort]\n",
    "\n",
    "f = plt.figure(figsize=(5, 10))\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "_ = ax.imshow(sortedws, interpolation='none')\n",
    "ax.set_title('recurrent weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
